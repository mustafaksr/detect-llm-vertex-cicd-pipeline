{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import fire\n",
    "import hypertune\n",
    "import time\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.environ[\"PROJECT_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb \"gs://{PROJECT_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}/detect-llm\"\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "JOB_DIR_ROOT = f\"{ARTIFACT_STORE}/jobs\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"detect_llm_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/training/train_df.csv\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/validation_df.csv\"\n",
    "TEST_FILE_PATH = f\"{DATA_ROOT}/test/test_df.csv\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JOB_DIR_ROOT\"] = JOB_DIR_ROOT\n",
    "os.environ[\"TRAINING_FILE_PATH\"] = TRAINING_FILE_PATH\n",
    "os.environ[\"VALIDATION_FILE_PATH\"] = VALIDATION_FILE_PATH\n",
    "os.environ[\"TEST_FILE_PATH\"] = TEST_FILE_PATH\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"JOB_NAME\"] = JOB_NAME\n",
    "os.environ[\"JOB_DIR\"] = JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigquery_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigquery_client.query(f\"SELECT * FROM `{PROJECT_ID}.detect_llm_ds_bq.raw_data` LIMIT 100\").result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.QueryJobConfig(destination=f\"{PROJECT_ID}.detect_llm_ds_bq.shuffle_raw\",write_disposition=\"WRITE_TRUNCATE\")\n",
    "sql = f'SELECT * \\\n",
    "FROM `{PROJECT_ID}.detect_llm_ds_bq.raw_data` ORDER BY RAND()'\n",
    "query_job = bigquery_client.query(sql, job_config=job_config)  \n",
    "query_job.result()  \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.QueryJobConfig(destination=f\"{PROJECT_ID}.detect_llm_ds_bq.training\",write_disposition=\"WRITE_TRUNCATE\")\n",
    "sql = f'SELECT * \\\n",
    "FROM `{PROJECT_ID}.detect_llm_ds_bq.shuffle_raw` AS train \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(train))), 12) IN (0, 2, 3, 4 ,5,6,7,8,9,10,11)'\n",
    "query_job = bigquery_client.query(sql, job_config=job_config)  \n",
    "query_job.result()  \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.QueryJobConfig(destination=f\"{PROJECT_ID}.detect_llm_ds_bq.test\",write_disposition=\"WRITE_TRUNCATE\")\n",
    "sql = f'SELECT * \\\n",
    "FROM `{PROJECT_ID}.detect_llm_ds_bq.shuffle_raw` AS train \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(train))), 12) IN (1,12)'\n",
    "query_job = bigquery_client.query(sql, job_config=job_config)  \n",
    "query_job.result()  \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.QueryJobConfig(destination=f\"{PROJECT_ID}.detect_llm_ds_bq.validation\",write_disposition=\"WRITE_TRUNCATE\")\n",
    "sql = f'SELECT * \\\n",
    "FROM `{PROJECT_ID}.detect_llm_ds_bq.training` AS train \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(train))), 12) IN (11,12)'\n",
    "query_job = bigquery_client.query(sql, job_config=job_config)  \n",
    "query_job.result()  \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.QueryJobConfig(destination=f\"{PROJECT_ID}.detect_llm_ds_bq.training\",write_disposition=\"WRITE_TRUNCATE\")\n",
    "sql = f'SELECT * \\\n",
    "FROM `{PROJECT_ID}.detect_llm_ds_bq.training` AS train \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(train))), 12) IN (0,1,2,3,4,5,6,7,8,9,10)'\n",
    "query_job = bigquery_client.query(sql, job_config=job_config)  \n",
    "query_job.result()  \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = PROJECT_ID\n",
    "dataset_id = \"detect_llm_ds_bq\"\n",
    "table_id = \"training\"\n",
    "destination_uri = TRAINING_FILE_PATH\n",
    "extract_job = bigquery_client.extract_table(\n",
    "    f\"{PROJECT_ID}.detect_llm_ds_bq.training\",\n",
    "    destination_uri,\n",
    "    # Location must match that of the source table.\n",
    "    location=\"US\",\n",
    ")  # API request\n",
    "extract_job.result()  # Waits for job to complete.\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = PROJECT_ID\n",
    "dataset_id = \"detect_llm_ds_bq\"\n",
    "table_id = \"validation\"\n",
    "destination_uri = VALIDATION_FILE_PATH\n",
    "extract_job = bigquery_client.extract_table(\n",
    "    f\"{PROJECT_ID}.detect_llm_ds_bq.validation\",\n",
    "    destination_uri,\n",
    "    # Location must match that of the source table.\n",
    "    location=\"US\",\n",
    ")  # API request\n",
    "extract_job.result()  # Waits for job to complete.\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = PROJECT_ID\n",
    "dataset_id = \"detect_llm_ds_bq\"\n",
    "table_id = \"test\"\n",
    "destination_uri = TEST_FILE_PATH\n",
    "extract_job = bigquery_client.extract_table(\n",
    "    f\"{PROJECT_ID}.detect_llm_ds_bq.test\",\n",
    "    destination_uri,\n",
    "    # Location must match that of the source table.\n",
    "    location=\"US\",\n",
    ")  # API request\n",
    "extract_job.result()  # Waits for job to complete.\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://$PROJECT_ID/detect-llm/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Create Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0 = pd.read_csv(TRAINING_FILE_PATH)\n",
    "val0 = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "test0 = pd.read_csv(TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val0.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2 \n",
    "dataset_tr = tf.data.Dataset.from_tensor_slices((train0.text.values,train0.label.values.astype(\"float32\") ))\n",
    "dataset_tr = dataset_tr.shuffle(buffer_size=len(train0)).batch(batch_size=2)\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((val0.text.values,val0.label.values.astype(\"float32\")  ))\n",
    "dataset_val = dataset_val.shuffle(buffer_size=len(val0)).batch(batch_size=2)\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((test0.text.values ))\n",
    "dataset_test = dataset_test.batch(batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "max_features = 5000\n",
    "embedding_dim = 64\n",
    "sequence_length = 64\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "text_ds = dataset_tr.map(lambda x, y: x).concatenate(dataset_val.map(lambda x, y: x)).concatenate(dataset_test)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "def vectorize_text_test(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text)\n",
    "train_ds = dataset_tr.map(vectorize_text)\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "\n",
    "val_ds = dataset_val.map(vectorize_text)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "\n",
    "test_ds = dataset_test.map(vectorize_text_test)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Conv1D(32, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(32, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "model = keras.Model(inputs, predictions)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "tf.random.set_seed(1)\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=epochs,workers=num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(test0.label,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.text[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "indices = vectorize_layer(inputs)\n",
    "outputs = model(indices)\n",
    "end_to_end_model = keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",tf.keras.metrics.AUC()]\n",
    ")\n",
    "end_to_end_preds = end_to_end_model.predict([test.text[123]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.label[123],end_to_end_preds.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_to_end_model.save(\"end_to_end_detect_llm.keras\",save_format=\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"end_to_end_detect_llm.keras\"\n",
    "                                      ,custom_objects={\"custom_standardization\":custom_standardization})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_to_end_model.predict([\"If you are someone who likes to go exploring and work a little then here is the job for you. The reason you should join this program is because on your extra time you can go sight seeing and look at what you will be passing on the jorney from place to place. The task you are asked to do is simple and you can do a variety of things on your way back home.\\n\\nIf you join this program then you could travel half way around the world, see different things, be in the millitary, help people, and work all at the same time. When you join this program you can see things like ruins and famos statues. Luke joined and look what he seen. He seen Europe, China, an excavated castle, and the Panama Canal. You don't see those things on an average day. If you join you can see all these things and have a life time expereince.\\n\\nWhen you are on this voage all you have to do is feed the animals and water them two or three times a day. The bales of hay and bags of oats had to be pulled up from the lower holds of the ship, so you might need some mucles. You also have to clean the stalls daily, which I know is a nasty job but someone has to do it. You also get the benefit of hellping families in need.\\n\\nOn the way back home you can also have fun by playing games with your friends in the empty stalls. It might not be the most sanatairy place but what else is there to do. Luke says that he and his friends play: Table-tennis, fencing, boxing,voley ball, base ball, and reading. This helped pass the time of the long jorney back home. This program could be fun even if you are in a ship and in animal stalls.\\n\\nThis why I think that joining this program would be good. You get to go sighting seeing, work, and have fun at the same time. Who would want to miss this opportunity of a life time like this one.         \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.predict([\"If you are someone who likes to go exploring and work a little then here is the job for you. The reason you should join this program is because on your extra time you can go sight seeing and look at what you will be passing on the jorney from place to place. The task you are asked to do is simple and you can do a variety of things on your way back home.\\n\\nIf you join this program then you could travel half way around the world, see different things, be in the millitary, help people, and work all at the same time. When you join this program you can see things like ruins and famos statues. Luke joined and look what he seen. He seen Europe, China, an excavated castle, and the Panama Canal. You don't see those things on an average day. If you join you can see all these things and have a life time expereince.\\n\\nWhen you are on this voage all you have to do is feed the animals and water them two or three times a day. The bales of hay and bags of oats had to be pulled up from the lower holds of the ship, so you might need some mucles. You also have to clean the stalls daily, which I know is a nasty job but someone has to do it. You also get the benefit of hellping families in need.\\n\\nOn the way back home you can also have fun by playing games with your friends in the empty stalls. It might not be the most sanatairy place but what else is there to do. Luke says that he and his friends play: Table-tennis, fencing, boxing,voley ball, base ball, and reading. This helped pass the time of the long jorney back home. This program could be fun even if you are in a ship and in animal stalls.\\n\\nThis why I think that joining this program would be good. You get to go sighting seeing, work, and have fun at the same time. Who would want to miss this opportunity of a life time like this one.         \"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Define Training Code and Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = \"training_app_trees\"\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import fire\n",
    "import hypertune\n",
    "import string\n",
    "import re\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "def train_evaluate(training_path,val_path,test_path,job_dir, hidden_dim, dropout, embedding_dim ,sequence_length ,max_features, hptune): \n",
    "    \n",
    "    train0 = pd.read_csv(training_path)\n",
    "    val0 = pd.read_csv(val_path)\n",
    "    test0 = pd.read_csv(test_path)\n",
    "    dataset_tr = tf.data.Dataset.from_tensor_slices((train0.text.values,train0.label.values.astype(\"float32\") ))\n",
    "    dataset_tr = dataset_tr.shuffle(buffer_size=len(train0)).batch(batch_size=2)\n",
    "    dataset_val = tf.data.Dataset.from_tensor_slices((val0.text.values,val0.label.values.astype(\"float32\")  ))\n",
    "    dataset_val = dataset_val.shuffle(buffer_size=len(val0)).batch(batch_size=2)\n",
    "    dataset_test = tf.data.Dataset.from_tensor_slices((test0.text.values ))\n",
    "    dataset_test = dataset_test.batch(batch_size=2)\n",
    "    def custom_standardization(input_data):\n",
    "        lowercase = tf.strings.lower(input_data)\n",
    "        stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "        return tf.strings.regex_replace(\n",
    "            stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "        )\n",
    "    max_features = int(max_features)\n",
    "    embedding_dim = int(embedding_dim)\n",
    "    sequence_length = int(sequence_length)\n",
    "    vectorize_layer = keras.layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=max_features,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=sequence_length,\n",
    "    )\n",
    "    text_ds = dataset_tr.map(lambda x, y: x).concatenate(dataset_val.map(lambda x, y: x)).concatenate(dataset_test)\n",
    "    vectorize_layer.adapt(text_ds)\n",
    "    \n",
    "    def vectorize_text(text, label):\n",
    "        text = tf.expand_dims(text, -1)\n",
    "        return vectorize_layer(text), label\n",
    "    def vectorize_text_test(text):\n",
    "        text = tf.expand_dims(text, -1)\n",
    "        return vectorize_layer(text)\n",
    "    train_ds = dataset_tr.map(vectorize_text)\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "    \n",
    "    val_ds = dataset_val.map(vectorize_text)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "    \n",
    "    test_ds = dataset_test.map(vectorize_text_test)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=10)\n",
    "    \n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "    x = layers.Dropout(float(dropout))(x)\n",
    "    x = layers.Conv1D(int(hidden_dim), 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    x = layers.Conv1D(int(hidden_dim), 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(int(hidden_dim), activation=\"relu\")(x)\n",
    "    x = layers.Dropout(float(dropout))(x)\n",
    "    \n",
    "    predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs, predictions)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",tf.keras.metrics.AUC()])\n",
    "    \n",
    "    num_cpus = os.cpu_count()\n",
    "    \n",
    "    epochs = 1\n",
    "    tf.random.set_seed(1)\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=epochs,workers=num_cpus)\n",
    "    \n",
    "    \n",
    "    preds = model.predict(test_ds)\n",
    "\n",
    "    if hptune:\n",
    "            roc_auc = roc_auc_score(test0.label,preds)\n",
    "            print('Model roc_auc: {}'.format(roc_auc))\n",
    "    \n",
    "            hpt = hypertune.HyperTune()\n",
    "            hpt.report_hyperparameter_tuning_metric(\n",
    "              hyperparameter_metric_tag='roc_auc',\n",
    "              metric_value=roc_auc\n",
    "            )\n",
    "    \n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = \"end_to_end_detect_llm\"\n",
    "        inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "        indices = vectorize_layer(inputs)\n",
    "        outputs = model(indices)\n",
    "        end_to_end_model = keras.Model(inputs, outputs)\n",
    "        end_to_end_model.compile(\n",
    "            loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",tf.keras.metrics.AUC()]\n",
    "        )\n",
    "        # end_to_end_model.save(model_filename,save_format=\"keras\")\n",
    "        tf.saved_model.save(\n",
    "        obj=end_to_end_model, export_dir=model_filename)\n",
    "    \n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', '-r', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "if __name__ == \"__main__\":\n",
    "        fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python training_app_trees/train.py --training_path $TRAINING_FILE_PATH --val_path $VALIDATION_FILE_PATH --test_path $TEST_FILE_PATH --job-dir $JOB_DIR --hidden_dim 32 --dropout 0.5 --embedding_dim 32 --sequence_length 128 --max_features 5000 --hptune False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $JOB_DIR/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil rm -r $JOB_DIR/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf-cpu.2-13\n",
    "RUN pip install -U pandas==2.1.4 numpy==1.24.3 fire cloudml-hypertune scikit-learn==1.3.2 fsspec gcsfs\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = \"detect_llm_trainer_image\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable cloudbuild.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Create Hyperparameter Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "CONFIG_YAML=config.yaml\n",
    "\n",
    "cat <<EOF > $CONFIG_YAML\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: roc_auc\n",
    "    goal: MAXIMIZE\n",
    "  parameters:\n",
    "  - parameterId: hidden_dim\n",
    "    discreteValueSpec:\n",
    "      values:\n",
    "      - 96\n",
    "      - 128\n",
    "      - 256\n",
    "  - parameterId: dropout\n",
    "    discreteValueSpec:\n",
    "      values:\n",
    "      - 0.1\n",
    "      - 0.2\n",
    "      - 0.5\n",
    "  - parameterId: embedding_dim\n",
    "    discreteValueSpec:\n",
    "      values:\n",
    "      - 96\n",
    "      - 128\n",
    "  - parameterId: sequence_length\n",
    "    discreteValueSpec:\n",
    "      values:\n",
    "      - 250\n",
    "      - 500\n",
    "  - parameterId: max_features\n",
    "    discreteValueSpec:\n",
    "      values:\n",
    "      - 15000\n",
    "      - 20000\n",
    "\n",
    "  algorithm: ALGORITHM_UNSPECIFIED # results in Bayesian optimization\n",
    "trialJobSpec:\n",
    "  workerPoolSpecs:  \n",
    "  - machineSpec:\n",
    "      machineType: $MACHINE_TYPE\n",
    "    replicaCount: $REPLICA_COUNT\n",
    "    containerSpec:\n",
    "      imageUri: $IMAGE_URI\n",
    "      args:\n",
    "      - --training_path=$TRAINING_FILE_PATH \n",
    "      - --val_path=$VALIDATION_FILE_PATH \n",
    "      - --test_path=$TEST_FILE_PATH \n",
    "      - --job-dir=$JOB_DIR \n",
    "      - --hptune\n",
    "EOF\n",
    "\n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=$CONFIG_YAML \\\n",
    "    --max-trial-count=1 \\\n",
    "    --parallel-trial-count=1\n",
    "\n",
    "echo \"JOB_NAME: $JOB_NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai hp-tuning-jobs describe YOUR_JOB_NUMBER --region=us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Create Custom Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform, bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials(job_name):\n",
    "    jobs = aiplatform.HyperparameterTuningJob.list()\n",
    "    match = [job for job in jobs if job.display_name == JOB_NAME]\n",
    "    tuning_job = match[0] if match else None\n",
    "    return tuning_job.trials if tuning_job else None\n",
    "\n",
    "\n",
    "def get_best_trial(trials):\n",
    "    metrics = [trial.final_measurement.metrics[0].value for trial in trials]\n",
    "    best_trial = trials[metrics.index(max(metrics))]\n",
    "    return best_trial\n",
    "\n",
    "\n",
    "def retrieve_best_trial_from_job_name(jobname):\n",
    "    trials = get_trials(jobname)\n",
    "    best_trial = get_best_trial(trials)\n",
    "    return best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = retrieve_best_trial_from_job_name(JOB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = best_trial.parameters[0].value\n",
    "embedding_dim = best_trial.parameters[1].value\n",
    "hidden_dim = best_trial.parameters[2].value\n",
    "max_features = best_trial.parameters[3].value\n",
    "sequence_length = best_trial.parameters[4].value\n",
    "score = best_trial.final_measurement.metrics[0].value\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"detect_llm_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "\n",
    "WORKER_POOL_SPEC = f\"\"\"\\\n",
    "machine-type={MACHINE_TYPE},\\\n",
    "replica-count={REPLICA_COUNT},\\\n",
    "container-image-uri={IMAGE_URI}\\\n",
    "\"\"\"\n",
    "\n",
    "ARGS = f\"\"\"\\\n",
    "--job_dir={JOB_DIR},\\\n",
    "--training_path={TRAINING_FILE_PATH},\\\n",
    "--val_path={VALIDATION_FILE_PATH},\\\n",
    "--test_path={VALIDATION_FILE_PATH},\\\n",
    "--dropout={dropout},\\\n",
    "--embedding_dim={embedding_dim},\\\n",
    "--hidden_dim={hidden_dim},\\\n",
    "--max_features={max_features},\\\n",
    "--sequence_length={sequence_length},\\\n",
    "--nohptune\\\n",
    "\"\"\"\n",
    "\n",
    "!gcloud ai custom-jobs create \\\n",
    "  --region={REGION} \\\n",
    "  --display-name={JOB_NAME} \\\n",
    "  --worker-pool-spec={WORKER_POOL_SPEC} \\\n",
    "  --args={ARGS}\n",
    "\n",
    "print(\"The model will be exported at:\", JOB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Deploy Model to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"detect_llm_classifier\"\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-13:latest\"\n",
    ")\n",
    "SERVING_MACHINE_TYPE = \"n1-standard-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_NAME,\n",
    "    artifact_uri= JOB_DIR+\"/end_to_end_detect_llm/\", # modelfile_name we defined in train.py\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = uploaded_model.deploy(\n",
    "    machine_type=SERVING_MACHINE_TYPE,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.predict([[\"Sample text to predict, this is not generated text but we need student article text to test.\"]])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_from_scratch",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4005256,
     "sourceId": 6977472,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
